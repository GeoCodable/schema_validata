# schema_validata

## Introduction
This library provides data validation functionalities for spreadsheets through validate_dataset(). It leverages a data dictionary (.xlsx) generated by build_data_dictionary and performs schema and custom regex pattern validation on your data sets.
A template (data_dictionary_template.xlsx) is provided for reference and as a starting point for manually building the data dictionary. 

### Key Features:

  - Validate Schemas from Spreadsheets:
    - Define your data's expected structure (data types, lengths, ranges, etc.) in a user-friendly data dictionary (.xlsx) generated by the included build_data_dictionary tool. This dictionary acts as your single source of truth for data validation rules.
  - Custom Regex Validation:
    - Go beyond basic schema checks by defining custom regex patterns directly within the data dictionary. This allows you to tailor validation to your specific data requirements, ensuring compliance with unique data formats or constraints.
  - Detailed Reports:
    - Obtain detailed JSON reports summarizing the validation results, including identified errors and metadata for both your data and the data dictionary. These reports provide valuable insights into data quality issues.
  - Optional Excel Reports:
    - For easier human interpretation, convert JSON reports into user-friendly Excel reports with the schema_validation_to_xlsx tool. This can be helpful for sharing and presenting validation results with colleagues or stakeholders.

## Installation
``` python
%pip install  git+https://github.com/ahamptonTIA/schema_validata.git
```
## General Workflow
### 1. Data Dictionary Creation:
  - Use build_data_dictionary (instructions to be provided separately) to generate a data dictionary (.xlsx) defining your data's expected schema. You can use the provided data_dictionary_template.xlsx as a starting point.
Define custom regex patterns within the data dictionary as specified in the template.
### 2. Schema Mapping:
  - Create a schema_mapping dictionary manually, associating dataset names/tabs with corresponding sheets.tabs in the data dictionary.
### 3. Data Validation:
  - Run validate_dataset with the data dictionary, schema mapping, and your actual data set (.csv or .xlsx). It will generate a JSON file containing detailed validation results, including schema and regex validation errors.
### 4. Report Generation:
  - Use schema_validation_to_xlsx to convert the JSON results into a human-readable Excel report (.xlsx) for easier interpretation.

## Function Descriptions

### 1. build_data_dictionary:

  - *Purpose*:
    - Analyzes a Pandas DataFrame to identify data types, null values, duplicates, unique values, allowed values, length, and potential value ranges for each column.  With an existing dataset, this can be a starting point for defining the data schema and generating a structured data dictionary (.xlsx) for validation processing as well as data goverance documentation.

  - *Parameters*:
    - df: The Pandas DataFrame to analyze.
    - max_unique_vals (optional): Maximum number of unique values to document in the allowed_value_list for string columns (default: 25).
    - false_val (optional): Value representing False (default: False).
    - true_val (optional): Value representing True (default: True).
    - na_val (optional): Value representing null/missing values (default: "N/A").
    - 
  - *Output*:
    - The function returns a dictionary (data_dict) where each key is a column name and the corresponding value is another dictionary containing column-level information:
      - data_type: Data type of the column (e.g. "String", "Float", "Datetime").
      - allow_null: Boolean indicating whether null values are allowed.
      - length: Maximum length of string or numeric digigts (e.g. String or Int ).
      - range_min: Minimum value (for numeric columns).
      - range_max: Maximum value (for numeric columns).
      - regex_pattern: Placeholder for user defined patterns.
      - unique_value: Boolean indicating whether the column has unique values.
      - allowed_value_list: Sorted list of observed allowed values
        - (for string columns with a manageable number of unique values as set by max_unique_vals).

### 2. validate_dataset:

  - *Purpose*:
    - Performs data validation against a defined schema and custom regex patterns. It leverages a data dictionary created by build_data_dictionary to ensure that data conforms to expectations and business rules to improve data quality and reliability.
      
  - *Supported Checks*:
    - Required fields
    - Data type checks
    - Allow null/missing value checks 
    - Uniqeness (duplicates)
    - Invalid max string or value lengths 
    - Ranges (minimum and maximum allowed values)
    - Allowed values lists
    - Customized regex pattern checks
      
  - *Parameters*:
    - dataset_path: Path to the data set (.csv or .xlsx).
    - data_dict_path: Path to the data dictionary (.xlsx).
    - schema_mapping: Dictionary associating dataset names/tabs with data dictionary sections.
    - out_dir (optional): Output directory for JSON results.
    - out_name (optional): Name for the JSON output file.

  - *Output*:
    - JSON file containing:
      - Dataset metadata.
      - Data dictionary metadata.
      - Schema validation errors.
      - Row level value errors.
  
### 3. schema_validation_to_xlsx:

  - *Purpose*:
    - Geneartes a user-friendly excel spreadsheet (.xlsx) for interpretation and presentation of data validation results. It takes the JSON report generated by validate_dataset and converts it into a human-readable Excel spreadsheet.
      
  - *Parameters*:
    - validation_results: JSON output from validate_dataset.
    - out_dir (optional): Output directory for the xlsx file.
    - out_name (optional): Name for the xlsx file.

  - *Output*:
    - User-friendly Excel report (.xlsx) with tabs/sheets:
      - Metadata overview
        - Metadata for the data dictioanry file as well as the dataset to be validated.
      - Schema error summary
        - A single tab containing high level errors for each dataset to be validated.
      - Individual value error details by dataset name (if applicable).
      - A tab/sheet for each dataset listing value errors with row and lookup indexing. 
